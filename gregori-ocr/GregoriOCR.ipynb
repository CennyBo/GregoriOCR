{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb69c09",
   "metadata": {},
   "source": [
    "# GregoriOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443e4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfa61a",
   "metadata": {},
   "source": [
    "0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c720c",
   "metadata": {},
   "source": [
    "    a. Download GregoBase data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005455e5",
   "metadata": {},
   "source": [
    "    b. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf0e63",
   "metadata": {},
   "source": [
    "        i. Use height of staves to scale image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f10e81",
   "metadata": {},
   "source": [
    "        ii. Extract gabc notes from training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97815e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#METHOD FOR PROFESSOR TO ACCESS THE DATASET\n",
    "\n",
    "#Download the dataset: (This is currently set for another project's dataset.)\n",
    "import platform\n",
    "mysystem = platform.system()\n",
    "file_id = '10PSeKeL3aUA56faRhr4ZfkEPcVtKjlry'\n",
    "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id\n",
    "# Check if system is Windows\n",
    "if mysystem != 'Windows':\n",
    "    !wget -O dataset.csv --no-check-certificate \"$file_download_link\"\n",
    "    # !unzip data.zip\n",
    "\n",
    "print('Please download the data using the following link:', file_download_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6141d",
   "metadata": {},
   "source": [
    "1. OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ce1eb",
   "metadata": {},
   "source": [
    "    a. Document Layout Analysis: Scanning function finds notes and puts them into individual boxes of uniform size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION THAT DETERMINES WHTHER SCANNING WINDOW CONTAINS A NOTE\n",
    "\n",
    "def is_note():\n",
    "    is_note = False\n",
    "    ...\n",
    "    \n",
    "    return is_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WINDOW THAT SCANS IMAGE FOR NOTES\n",
    "\n",
    "#Crop a subset of an image (about the height of a staff and about the width of a note).\n",
    "#Use function to determine whether this is a note.\n",
    "#If it is a note:\n",
    "    #Add the image to a list\n",
    "    #Move window to the right by the width of one note\n",
    "#Else:\n",
    "    #Move the window one pixel to the right\n",
    "\n",
    "def scan_window():\n",
    "    notes = []\n",
    "    ...\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac0491",
   "metadata": {},
   "source": [
    "    b. Keras CNN to perform OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE DATASET\n",
    "\n",
    "#This is taken from my ECS 171 homework assignment that uses neural networks to determine whther an image is or isn't a cat.\n",
    "#Those images had been converted to numpy (.npz) files beforehand.\n",
    "#Who knows if we'll do that, but it's nice for a general idea.\n",
    "\n",
    "data = np.load('./data/cats/cats.npz') #This is set for a different dataset\n",
    "\n",
    "#For looking at the file contents\n",
    "lst = data.files\n",
    "for item in lst:\n",
    "    print(item)\n",
    "    print(data[item])\n",
    "\n",
    "\n",
    "X_train, y_train = data['Xtrain'].transpose(), data['Ytrain'].transpose()\n",
    "X_test, y_test = data['Xtest'].transpose(), data['Ytest'].transpose()\n",
    "\n",
    "#Save original shapes of X_train and X_test in case we want them later.\n",
    "X_train_orig = X_train\n",
    "X_test_orig = X_test\n",
    "\n",
    "#Reshape the data from a 1D vector to a 2D matrix.\n",
    "#This represents a grid of pixel coordinates.\n",
    "#X_train.shape[0] returns our number of observations in X_train.\n",
    "#Parameters of .reshape():\n",
    "    #.reshape([number of observations in dataset, height in pixels, width in pixels, number of color dimensions])\n",
    "X_train = X_train.reshape([X_train.shape[0], 64, 64, 1])\n",
    "\n",
    "\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "display(X_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE-HOT ENCODE y_train\n",
    "\n",
    "#Use one-hot encoding to transform y_train into a matrix where each column represents a different possible outcome.\n",
    "#This allows the model to predict more than 2 classes.\n",
    "#In our case, y_train will be m x n, where m is the number of observations and n is the number of possible notes/outcomes.\n",
    "\n",
    "y_train_single = y_train\n",
    "one_hot_encoding = pd.get_dummies(y_train)\n",
    "y_train = one_hot_encoding\n",
    "\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14715816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVOLUSIONAL NEURAL NETWORK\n",
    "\n",
    "model = Sequential() #Initialize the model.\n",
    "\n",
    "#LAYERS:\n",
    "#Convulusional layers:\n",
    "#Supposedly selu is better than relu.  It's worth testing.\n",
    "model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "\n",
    "#Always flatten inputs after you convulusional layers are finished:\n",
    "model.add(Flatten())\n",
    "\n",
    "#Non-convolusional layers:\n",
    "model.add(Dense(units=30, activation='selu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=15, activation='selu'))\n",
    "\n",
    "#Output Layer:\n",
    "#n := number of possible predictions/notes (88?)\n",
    "model.add(Dense(units=n, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILE THE MODEL\n",
    "\n",
    "#ECS 171 professor says adam is awesome optimizer.\n",
    "#We use categorical rather than binary cross entropy because we are predicting more than 2 classes.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8689ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN THE MODEL\n",
    "\n",
    "model.fit(X_train.astype('float'), y_train, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb145c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT TEST DATA\n",
    "\n",
    "yhat_test = model.predict(X_test.astype(float))\n",
    "\n",
    "#The output will be n values.\n",
    "#We take the outcome associated with the the largest value as our result/prediction.\n",
    "yhat = []\n",
    "for y in yhat_test:\n",
    "    yhat.append(np.argmax(y)) #Outcome associated with the largest value.\n",
    "yhat = np.array(yhat)\n",
    "\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398ff28",
   "metadata": {},
   "source": [
    "2. Accuracy Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127aa449",
   "metadata": {},
   "source": [
    "Compare our NN's output with the gabc file from GregoBase.\n",
    "Note: In gabc files, the notes are within parentheses, so we can simply pick them out.\n",
    "Our output will be a list of notes represented as strings.\n",
    "\n",
    "The accuracy function simply compares our list with the notes present in the gabc.\n",
    "There are many ways to do this comparison.\n",
    "\n",
    "The simplest is to compare the first note in our list with the first note in the gabc, then the second note in our list with the second note in the gabc, and so on.\n",
    "The flaw with this is that if our scanning window misses a note, our whole accuracy measure will be thrown off.\n",
    "\n",
    "Another way to do this is to check the quantity of each type of note in the original gabc file and see if it matches the quantity in our own output.\n",
    "E.g., there are 12 g notes in the original, did our neural net find 12 g notes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6df7fc",
   "metadata": {},
   "source": [
    "3. Gregorio Format: Output predictions as a gabc file\n",
    "\n",
    "Pretty simple, write each string/note in our output list to a gabc file.\n",
    "Also, enclose each note in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12112bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gabc():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
