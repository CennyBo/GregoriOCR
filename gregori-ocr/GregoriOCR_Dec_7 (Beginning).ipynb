{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb69c09",
   "metadata": {},
   "source": [
    "# GregoriOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4bfbb",
   "metadata": {},
   "source": [
    "### 0. Download gregorio data.\n",
    "### 1. Convert image to numpy array.\n",
    "### 2. Use thin strip to scan across image looking for notes.\n",
    "    a. The strip is the height of the image and 1 pixel wide.\n",
    "    b. If the strip sees a \"tower\" of dark pixels (taller than some threshold height, e.g., 10 pixels tall), that's a potential note.\n",
    "    c. If the subsequent strips also see \"towers\" of dark pixels at that same height, we classify decide a note is there.  (E.g., we had 10 strips in a row all see a tower of dark pixels at the same height, so we decide that there must be a note there.)\n",
    "### 3. Calculate the center pixel of the note we found.\n",
    "### 4. Classify double and triple notes.\n",
    "    a. When we find two notes that are very close together, we classify them as a double note.\n",
    "    b. E.g., our threshold is 10, and two centers are at (20, 56) and (25, 83).  25 - 20 = 5 < 10, so the notes are very close together and must be double notes.\n",
    "    c. To classify triple notes, we just need to look one note further.  If we find another note that is very close to the double note, then we classify that as a triple note.\n",
    "### 5. Find pitch of first note in staff.\n",
    "    a. Use CNN to find pitch of that note?\n",
    "    b. If we use a CNN, we would cut out a window around the first note.  (The window would be the height of the image and the width of the note.  Use CNN to classify that note.\n",
    "    c. Note: To make this easier for the CNN, if the first note is a double note, we can instead use the first single note in the staff.\n",
    "### 6. Classify pitches of all other notes based on relative height to first note.\n",
    "    a. We decide some threshold distance for when two notes are at different pitches, e.g., 10 pixel height difference.\n",
    "    b. E.g., first note is at (20, 50) and second note is at (20, 80).  80 - 50 = 30, so second note is 3 pitches above first note.\n",
    "### 7. Classify note types.\n",
    "    a. Cut out square around each center, and use CNN to classify note type.\n",
    "### 8. Combine pitch and note type information to make list of all notes in staff.\n",
    "### 9. Convert list to gabc output.\n",
    "### 10. Evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443e4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259d21d",
   "metadata": {},
   "source": [
    "# 1. Convert image to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"image.png\")\n",
    "arr = np.array(img)\n",
    "\n",
    "#Old:\n",
    "#data = np.load('./data/cats/cats.npz') #This is set for a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c930da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For looking at the file contents\n",
    "lst = data.files\n",
    "for item in lst:\n",
    "    print(item)\n",
    "    print(data[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data['Xtrain'].transpose(), data['Ytrain'].transpose()\n",
    "X_test, y_test = data['Xtest'].transpose(), data['Ytest'].transpose()\n",
    "\n",
    "#Reshape the data from a 1D vector to a 2D matrix.\n",
    "#This represents a grid of pixel coordinates.\n",
    "#X_train.shape[0] returns our number of observations in X_train.\n",
    "#Parameters of .reshape():\n",
    "    #.reshape([number of observations in dataset, height in pixels, width in pixels, number of color dimensions])\n",
    "X_train = X_train.reshape([X_train.shape[0], 64, 64, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "display(X_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE-HOT ENCODE y_train\n",
    "\n",
    "#Use one-hot encoding to transform y_train into a matrix where each column represents a different possible outcome.\n",
    "#This allows the model to predict more than 2 classes.\n",
    "#In our case, y_train will be m x n, where m is the number of observations and n is the number of possible outcomes (note types).\n",
    "\n",
    "one_hot_encoding = pd.get_dummies(y_train)\n",
    "y_train = one_hot_encoding\n",
    "\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6e755",
   "metadata": {},
   "source": [
    "# 2. Use thin strip to scan across image looking for notes.\n",
    "a. The strip is the height of the image and 1 pixel wide.\n",
    "\n",
    "b. If the strip sees a \"tower\" of dark pixels (taller than some threshold height, e.g., 10 pixels tall), that's a potential note.\n",
    "\n",
    "c. If the subsequent strips also see \"towers\" of dark pixels at that same height, we classify decide a note is there.  (E.g., we had 10 strips in a row all see a tower of dark pixels at the same height, so we decide that there must be a note there.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757eaee",
   "metadata": {},
   "source": [
    "# 3. Calculate the center pixel of the note we found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a593c",
   "metadata": {},
   "source": [
    "# 4. Classify double and triple notes.\n",
    "a. When we find two notes that are very close together, we classify them as a double note.\n",
    "\n",
    "b. E.g., our threshold is 10, and two centers are at (20, 56) and (25, 83).  25 - 20 = 5 < 10, so the notes are very close together and must be double notes.\n",
    "\n",
    "c. To classify triple notes, we just need to look one note further.  If we find another note that is very close to the double note, then we classify that as a triple note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59229b0",
   "metadata": {},
   "source": [
    "# 5. Find pitch of first note in staff.\n",
    "a. Use CNN to find pitch of that note?\n",
    "\n",
    "b. If we use a CNN, we would cut out a window around the first note.  (The window would be the height of the image and the width of the note.  Use CNN to classify that note.\n",
    "\n",
    "c. Note: To make this easier for the CNN, if the first note is a double note, we can instead use the first single note in the staff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a608e0",
   "metadata": {},
   "source": [
    "# 6. Classify pitches of all other notes based on relative height to first note.\n",
    "a. We decide some threshold distance for when two notes are at different pitches, e.g., 10 pixel height difference.\n",
    "\n",
    "b. E.g., first note is at (20, 50) and second note is at (20, 80).  80 - 50 = 30, so second note is 3 pitches above first note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d4ccf",
   "metadata": {},
   "source": [
    "# 7. Classify note types.\n",
    "a. Cut out square around each center, and use CNN to classify note type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf1b2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2375282020.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 17\u001b[1;36m\u001b[0m\n\u001b[1;33m    model.add(Dense(units=, activation='softmax'))\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "\n",
    "#LAYERS:\n",
    "#Convulusional layers:\n",
    "#Supposedly selu is better than relu.  It's worth testing.\n",
    "model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "\n",
    "#Always flatten inputs after you convulusional layers are finished:\n",
    "model.add(Flatten())\n",
    "\n",
    "#Non-convolusional layers:\n",
    "model.add(Dense(units=30, activation='selu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=15, activation='selu'))\n",
    "\n",
    "#Output Layer:\n",
    "model.add(Dense(units=, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILE THE MODEL\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN THE MODEL\n",
    "model.fit(X_train.astype('float'), y_train, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95bb248c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NoN_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#PREDICT TEST DATA\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m yhat_test \u001b[38;5;241m=\u001b[39m NoN_model\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#We take the outcome associated with the the largest value as our result/prediction.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m yhat \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NoN_model' is not defined"
     ]
    }
   ],
   "source": [
    "#PREDICT TEST DATA\n",
    "\n",
    "yhat_test = model.predict(X_test.astype(float))\n",
    "\n",
    "#We take the outcome associated with the the largest value as our result/prediction.\n",
    "yhat = []\n",
    "for y in yhat_test:\n",
    "    yhat.append(np.argmax(y)) #Outcome associated with the largest value.\n",
    "yhat = np.array(yhat)\n",
    "\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecbddcc",
   "metadata": {},
   "source": [
    "# 8. Combine pitch and note type information to make list of all notes in staff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c4a4c",
   "metadata": {},
   "source": [
    "# 9. Convert list to gabc output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398ff28",
   "metadata": {},
   "source": [
    "# 10. Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127aa449",
   "metadata": {},
   "source": [
    "Compare our NN's output with the gabc file from GregoBase.\n",
    "Note: In gabc files, the notes are within parentheses, so we can simply pick them out.\n",
    "Our output will be a list of notes represented as strings.\n",
    "\n",
    "The accuracy function simply compares our list with the notes present in the gabc.\n",
    "There are many ways to do this comparison.\n",
    "\n",
    "The simplest is to compare the first note in our list with the first note in the gabc, then the second note in our list with the second note in the gabc, and so on.\n",
    "The flaw with this is that if our scanning window misses a note, our whole accuracy measure will be thrown off.\n",
    "\n",
    "Another way to do this is to check the quantity of each type of note in the original gabc file and see if it matches the quantity in our own output.\n",
    "E.g., there are 12 g notes in the original, did our neural net find 12 g notes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6df7fc",
   "metadata": {},
   "source": [
    "# Write to gabc File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12112bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0bfaa7c",
   "metadata": {},
   "source": [
    "# (Optional) Staff Line Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6401f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_model = Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c2fcb",
   "metadata": {},
   "source": [
    "# (Optional) Find dsl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
