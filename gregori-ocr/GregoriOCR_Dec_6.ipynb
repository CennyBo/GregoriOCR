{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb69c09",
   "metadata": {},
   "source": [
    "# GregoriOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4bfbb",
   "metadata": {},
   "source": [
    "0. Download some gregorio data\n",
    "1. staff line height\n",
    "2. staff line locating\n",
    "3. note recognition\n",
    "    a. \n",
    "4. gabc output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443e4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97815e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#METHOD FOR PROFESSOR TO ACCESS THE DATASET\n",
    "\n",
    "#Download the dataset: (This is currently set for another project's dataset.)\n",
    "import platform\n",
    "mysystem = platform.system()\n",
    "file_id = '10PSeKeL3aUA56faRhr4ZfkEPcVtKjlry'\n",
    "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id\n",
    "# Check if system is Windows\n",
    "if mysystem != 'Windows':\n",
    "    !wget -O dataset.csv --no-check-certificate \"$file_download_link\"\n",
    "    # !unzip data.zip\n",
    "\n",
    "print('Please download the data using the following link:', file_download_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259d21d",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE DATASET\n",
    "\n",
    "#This is taken from my ECS 171 homework assignment that uses neural networks to determine whther an image is or isn't a cat.\n",
    "#Those images had been converted to numpy (.npz) files beforehand.\n",
    "#Who knows if we'll do that, but it's nice for a general idea.\n",
    "\n",
    "data = np.load('./data/cats/cats.npz') #This is set for a different dataset\n",
    "\n",
    "#For looking at the file contents\n",
    "lst = data.files\n",
    "for item in lst:\n",
    "    print(item)\n",
    "    print(data[item])\n",
    "\n",
    "\n",
    "X_train, y_train = data['Xtrain'].transpose(), data['Ytrain'].transpose()\n",
    "X_test, y_test = data['Xtest'].transpose(), data['Ytest'].transpose()\n",
    "\n",
    "#Save original shapes of X_train and X_test in case we want them later.\n",
    "X_train_orig = X_train\n",
    "X_test_orig = X_test\n",
    "\n",
    "#Reshape the data from a 1D vector to a 2D matrix.\n",
    "#This represents a grid of pixel coordinates.\n",
    "#X_train.shape[0] returns our number of observations in X_train.\n",
    "#Parameters of .reshape():\n",
    "    #.reshape([number of observations in dataset, height in pixels, width in pixels, number of color dimensions])\n",
    "X_train = X_train.reshape([X_train.shape[0], 64, 64, 1])\n",
    "\n",
    "\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "display(X_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE-HOT ENCODE y_train\n",
    "\n",
    "#Use one-hot encoding to transform y_train into a matrix where each column represents a different possible outcome.\n",
    "#This allows the model to predict more than 2 classes.\n",
    "#In our case, y_train will be m x n, where m is the number of observations and n is the number of possible notes/outcomes.\n",
    "\n",
    "y_train_single = y_train\n",
    "one_hot_encoding = pd.get_dummies(y_train)\n",
    "y_train = one_hot_encoding\n",
    "\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174304a7",
   "metadata": {},
   "source": [
    "# Mark Locations of All Notes on Staff:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcd9df",
   "metadata": {},
   "source": [
    "# Function to Construct NoN_y:\n",
    "Read gabc file.\n",
    "Find length of each note.\n",
    "Append length to NoN_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NoN_y(gabc_file):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c340700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/test split\n",
    "X_train, X_test, NoN_y_train, NoN_y_test = train_test_split(df_norm.drop(['popularity'], axis=1), df_norm.popularity, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7aff75",
   "metadata": {},
   "source": [
    "# Number-of-Notes Function:\n",
    "Measures x-distance between all neighboring notes.\n",
    "If distance is less than some threshold, notes are classified as a double note.\n",
    "Also, keep track of status of previous note, so you can classify triple notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NoN_model = Sequential() \n",
    "\n",
    "#LAYERS:\n",
    "#Convulusional layers:\n",
    "#Supposedly selu is better than relu.  It's worth testing.\n",
    "NoN_model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "NoN_model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "\n",
    "#Always flatten inputs after you convulusional layers are finished:\n",
    "NoN_model.add(Flatten())\n",
    "\n",
    "#Non-convolusional layers:\n",
    "NoN_model.add(Dense(units=30, activation='selu', input_dim=X_train.shape[1]))\n",
    "NoN_model.add(Dense(units=15, activation='selu'))\n",
    "\n",
    "#Output Layer:\n",
    "possibile_NoN = 2\n",
    "NoN_model.add(Dense(units=possible_NoN, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4153494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILE THE MODEL\n",
    "NoN_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2138dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN THE MODEL\n",
    "NoN_model.fit(X_train.astype('float'), NoN_y_train, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT TEST DATA\n",
    "\n",
    "NoN_yhat_test = NoN_model.predict(X_test.astype(float))\n",
    "\n",
    "#The output will be 2 values.\n",
    "#We take the outcome associated with the the largest value as our result/prediction.\n",
    "NoN_yhat = []\n",
    "for y in NoN_yhat_test:\n",
    "    NoN_yhat.append(np.argmax(y)) #Outcome associated with the largest value.\n",
    "NoN_yhat = np.array(NoN_yhat)\n",
    "\n",
    "print(NoN_yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb2011",
   "metadata": {},
   "source": [
    "# Pitch Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14715816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pitch classifier\n",
    "snp_model = Sequential() #Initialize the model.\n",
    "\n",
    "#Convulusional layers:\n",
    "pitch_model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "pitch_model.add(Conv2D(32, (3, 3), activation='selu'))\n",
    "\n",
    "pitch_model.add(Flatten())\n",
    "\n",
    "#Non-convolusional layers:\n",
    "pitch_model.add(Dense(units=30, activation='selu', input_dim=X_train.shape[1]))\n",
    "pitch_model.add(Dense(units=15, activation='selu'))\n",
    "\n",
    "#Output Layer:\n",
    "#13 possible pitches\n",
    "pitch_model.add(Dense(units=13, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8689ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_model.fit(X_train.astype('float'), y_train, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb145c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT\n",
    "yhat_test = snp_model.predict(X_test.astype(float))\n",
    "\n",
    "#The output will be n values.\n",
    "#We take the outcome associated with the the largest value as our result/prediction.\n",
    "yhat = []\n",
    "for y in yhat_test:\n",
    "    yhat.append(np.argmax(y)) #Outcome associated with the largest value.\n",
    "yhat = np.array(yhat)\n",
    "\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f433f4b",
   "metadata": {},
   "source": [
    "# Function to Measure Difference in Note Heights:\n",
    "Used for double notes.\n",
    "Measures difference in height between the first and second note.\n",
    "E.g., if second note is 3 pitches up, we know the second note is the pitch of the first note + 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cf4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e398ff28",
   "metadata": {},
   "source": [
    "# Accuracy Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127aa449",
   "metadata": {},
   "source": [
    "Compare our NN's output with the gabc file from GregoBase.\n",
    "Note: In gabc files, the notes are within parentheses, so we can simply pick them out.\n",
    "Our output will be a list of notes represented as strings.\n",
    "\n",
    "The accuracy function simply compares our list with the notes present in the gabc.\n",
    "There are many ways to do this comparison.\n",
    "\n",
    "The simplest is to compare the first note in our list with the first note in the gabc, then the second note in our list with the second note in the gabc, and so on.\n",
    "The flaw with this is that if our scanning window misses a note, our whole accuracy measure will be thrown off.\n",
    "\n",
    "Another way to do this is to check the quantity of each type of note in the original gabc file and see if it matches the quantity in our own output.\n",
    "E.g., there are 12 g notes in the original, did our neural net find 12 g notes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6df7fc",
   "metadata": {},
   "source": [
    "# Write to gabc File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12112bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0bfaa7c",
   "metadata": {},
   "source": [
    "# (Optional) Staff Line Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6401f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_model = Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c2fcb",
   "metadata": {},
   "source": [
    "# (Optional) Find dsl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
